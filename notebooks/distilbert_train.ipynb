{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сетап"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, get_scheduler, default_data_collator\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import collections\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"kuznetsoffandrey/sberquad\"\n",
    "MODEL_NAME = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "MODEL_SAVE_DIR ='distilbert-v2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка датасета и его предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 45328\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5036\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 23936\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 60544,\n",
       " 'title': 'SberChallenge',\n",
       " 'context': 'Первые упоминания о строении человеческого тела встречаются в Древнем Египте. В XXVII веке до н. э. египетский врач Имхотеп описал некоторые органы и их функции, в частности головной мозг, деятельность сердца, распространение крови по сосудам. В древнекитайской книге Нейцзин (XI—VII вв. до н. э.) упоминаются сердце, печень, лёгкие и другие органы тела человека. В индийской книге Аюрведа ( Знание жизни , IX-III вв. до н. э.) содержится большой объём анатомических данных о мышцах, нервах, типах телосложения и темперамента, головном и спинном мозге.',\n",
       " 'question': 'Где встречаются первые упоминания о строении человеческого тела?',\n",
       " 'answers': {'text': ['в Древнем Египте'], 'answer_start': [60]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 18009,\n",
       " 'title': 'SberChallenge',\n",
       " 'context': 'Многоклеточный организм — внесистематическая категория живых организмов, тело которых состоит из многих клеток, большая часть которых (кроме стволовых, например, клеток камбия у растений) дифференцированы, то есть различаются по строению и выполняемым функциям. Следует отличать многоклеточность и колониальность. У колониальных организмов отсутствуют настоящие дифференцированные клетки, а следовательно, и разделение тела на ткани. Граница между многоклеточностью и колониальностью нечёткая. Например, вольвокс часто относят к колониальным организмам, хотя в его колониях есть чёткое деление клеток на генеративные и соматические. Кроме дифференциации клеток, для многоклеточных характерен и более высокий уровень интеграции, чем для колониальных форм. Многоклеточные животные, возможно, появились на Земле 2,1 миллиарда лет назад, вскоре после кислородной революции .',\n",
       " 'question': 'У каких организмов отсутствуют настоящие дифференцированные клетки?',\n",
       " 'answers': {'text': [''], 'answer_start': [-1]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\python_projects\\ru_qa_system\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 45383, 10990, 42128, 553, 26891, 565, 11752, 46559, 136, 102, 521, 26891, 565, 11752, 46559, 10277, 14315, 118, 18261, 14315, 118, 32034, 105856, 10205, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'offset_mapping': [[(0, 0), (0, 4), (4, 6), (7, 12), (13, 14), (14, 16), (17, 18), (18, 20), (20, 23), (23, 24), (0, 0), (0, 1), (1, 3), (4, 5), (5, 7), (7, 10), (11, 13), (13, 15), (15, 16), (16, 18), (18, 20), (20, 21), (21, 25), (25, 29), (29, 30), (30, 31), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0), (0, 0)]], 'overflow_to_sample_mapping': [0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\"Какого цвета мои шнурки?\", \"Мои шнурки серо-буро-малиновые.\",\n",
    "        max_length=50,\n",
    "        truncation=\"only_second\",\n",
    "        stride=30,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenized_example.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, tokenizer, is_test=False, max_length=384, stride=128):\n",
    "    questions = [q.strip() for q in examples['question']]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    answers = examples[\"answers\"]\n",
    "    if is_test:\n",
    "        offset_mapping = inputs[\"offset_mapping\"]\n",
    "        example_ids = []\n",
    "    else:\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        offset = offset_mapping[i]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        if is_test:\n",
    "            example_ids.append(examples[\"id\"][sample_idx])\n",
    "            inputs[\"offset_mapping\"][i] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "\n",
    "        if len(answer['answer_start'])==0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "        \n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        \n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "            \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    if is_test:\n",
    "        inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619b098a15cd4a6bb693798a8c4dc4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/45328 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_train = dataset['train'].map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    fn_kwargs = {\n",
    "        'tokenizer': tokenizer,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 47782\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37b7f8ca3e541d881460cca49927204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5036 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_val = dataset['validation'].map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['validation'].column_names,\n",
    "    fn_kwargs = {\n",
    "        'tokenizer': tokenizer,\n",
    "        'is_test': True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'offset_mapping', 'start_positions', 'end_positions', 'example_id'],\n",
       "    num_rows: 5316\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  512,\n",
       "  12265,\n",
       "  78007,\n",
       "  37067,\n",
       "  560,\n",
       "  91363,\n",
       "  13139,\n",
       "  12268,\n",
       "  555,\n",
       "  19240,\n",
       "  102875,\n",
       "  11905,\n",
       "  25877,\n",
       "  16030,\n",
       "  19069,\n",
       "  33045,\n",
       "  136,\n",
       "  102,\n",
       "  59186,\n",
       "  560,\n",
       "  91363,\n",
       "  13139,\n",
       "  12268,\n",
       "  555,\n",
       "  19240,\n",
       "  102875,\n",
       "  11905,\n",
       "  25877,\n",
       "  16030,\n",
       "  19069,\n",
       "  33045,\n",
       "  78007,\n",
       "  543,\n",
       "  513,\n",
       "  110442,\n",
       "  30072,\n",
       "  514,\n",
       "  14122,\n",
       "  11078,\n",
       "  10696,\n",
       "  119,\n",
       "  511,\n",
       "  102974,\n",
       "  30025,\n",
       "  32479,\n",
       "  10344,\n",
       "  554,\n",
       "  119,\n",
       "  570,\n",
       "  119,\n",
       "  546,\n",
       "  14122,\n",
       "  107006,\n",
       "  11386,\n",
       "  95739,\n",
       "  517,\n",
       "  10241,\n",
       "  42940,\n",
       "  86121,\n",
       "  555,\n",
       "  75356,\n",
       "  30847,\n",
       "  79987,\n",
       "  549,\n",
       "  12064,\n",
       "  44490,\n",
       "  117,\n",
       "  543,\n",
       "  27184,\n",
       "  64371,\n",
       "  11075,\n",
       "  553,\n",
       "  44666,\n",
       "  10823,\n",
       "  117,\n",
       "  34112,\n",
       "  10277,\n",
       "  23479,\n",
       "  11456,\n",
       "  117,\n",
       "  98826,\n",
       "  83191,\n",
       "  10297,\n",
       "  10956,\n",
       "  16417,\n",
       "  41127,\n",
       "  119,\n",
       "  511,\n",
       "  16522,\n",
       "  13292,\n",
       "  10695,\n",
       "  10648,\n",
       "  25987,\n",
       "  11106,\n",
       "  48658,\n",
       "  21124,\n",
       "  10384,\n",
       "  12181,\n",
       "  47397,\n",
       "  113,\n",
       "  14627,\n",
       "  100,\n",
       "  12988,\n",
       "  60345,\n",
       "  119,\n",
       "  10344,\n",
       "  554,\n",
       "  119,\n",
       "  570,\n",
       "  119,\n",
       "  114,\n",
       "  560,\n",
       "  91363,\n",
       "  13139,\n",
       "  17601,\n",
       "  10277,\n",
       "  92674,\n",
       "  117,\n",
       "  556,\n",
       "  63077,\n",
       "  15266,\n",
       "  117,\n",
       "  552,\n",
       "  25913,\n",
       "  10823,\n",
       "  27441,\n",
       "  549,\n",
       "  19842,\n",
       "  79987,\n",
       "  33045,\n",
       "  18035,\n",
       "  119,\n",
       "  511,\n",
       "  27796,\n",
       "  37066,\n",
       "  11106,\n",
       "  48658,\n",
       "  509,\n",
       "  10593,\n",
       "  106116,\n",
       "  10987,\n",
       "  113,\n",
       "  516,\n",
       "  83095,\n",
       "  17237,\n",
       "  117,\n",
       "  14338,\n",
       "  118,\n",
       "  10652,\n",
       "  60345,\n",
       "  119,\n",
       "  10344,\n",
       "  554,\n",
       "  119,\n",
       "  570,\n",
       "  119,\n",
       "  114,\n",
       "  64570,\n",
       "  10625,\n",
       "  31308,\n",
       "  13248,\n",
       "  76819,\n",
       "  69864,\n",
       "  12378,\n",
       "  45433,\n",
       "  18070,\n",
       "  34103,\n",
       "  555,\n",
       "  35818,\n",
       "  11148,\n",
       "  92308,\n",
       "  117,\n",
       "  10375,\n",
       "  74079,\n",
       "  10353,\n",
       "  117,\n",
       "  21798,\n",
       "  10353,\n",
       "  50235,\n",
       "  55984,\n",
       "  22482,\n",
       "  549,\n",
       "  19710,\n",
       "  92274,\n",
       "  57772,\n",
       "  117,\n",
       "  64371,\n",
       "  12391,\n",
       "  549,\n",
       "  558,\n",
       "  20785,\n",
       "  46625,\n",
       "  10241,\n",
       "  553,\n",
       "  44666,\n",
       "  12670,\n",
       "  119,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'offset_mapping': [None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  [0, 6],\n",
       "  [7, 8],\n",
       "  [8, 11],\n",
       "  [11, 14],\n",
       "  [14, 17],\n",
       "  [18, 19],\n",
       "  [20, 23],\n",
       "  [23, 26],\n",
       "  [26, 28],\n",
       "  [29, 32],\n",
       "  [32, 35],\n",
       "  [35, 42],\n",
       "  [43, 47],\n",
       "  [48, 59],\n",
       "  [60, 61],\n",
       "  [62, 63],\n",
       "  [63, 66],\n",
       "  [66, 69],\n",
       "  [70, 71],\n",
       "  [71, 73],\n",
       "  [73, 74],\n",
       "  [74, 76],\n",
       "  [76, 77],\n",
       "  [78, 79],\n",
       "  [80, 83],\n",
       "  [83, 85],\n",
       "  [86, 90],\n",
       "  [91, 93],\n",
       "  [94, 95],\n",
       "  [95, 96],\n",
       "  [97, 98],\n",
       "  [98, 99],\n",
       "  [100, 101],\n",
       "  [101, 103],\n",
       "  [103, 106],\n",
       "  [106, 110],\n",
       "  [111, 115],\n",
       "  [116, 117],\n",
       "  [117, 118],\n",
       "  [118, 120],\n",
       "  [120, 123],\n",
       "  [124, 125],\n",
       "  [125, 130],\n",
       "  [131, 140],\n",
       "  [141, 147],\n",
       "  [148, 149],\n",
       "  [150, 152],\n",
       "  [153, 160],\n",
       "  [160, 161],\n",
       "  [162, 163],\n",
       "  [164, 173],\n",
       "  [174, 179],\n",
       "  [179, 182],\n",
       "  [183, 184],\n",
       "  [184, 186],\n",
       "  [186, 187],\n",
       "  [187, 188],\n",
       "  [189, 201],\n",
       "  [202, 204],\n",
       "  [204, 206],\n",
       "  [206, 208],\n",
       "  [208, 209],\n",
       "  [210, 225],\n",
       "  [226, 231],\n",
       "  [232, 234],\n",
       "  [235, 237],\n",
       "  [237, 239],\n",
       "  [239, 242],\n",
       "  [242, 243],\n",
       "  [244, 245],\n",
       "  [246, 248],\n",
       "  [248, 250],\n",
       "  [250, 252],\n",
       "  [252, 254],\n",
       "  [254, 257],\n",
       "  [257, 261],\n",
       "  [262, 267],\n",
       "  [268, 270],\n",
       "  [270, 271],\n",
       "  [271, 272],\n",
       "  [272, 275],\n",
       "  [276, 277],\n",
       "  [277, 279],\n",
       "  [279, 280],\n",
       "  [280, 283],\n",
       "  [284, 286],\n",
       "  [286, 287],\n",
       "  [288, 290],\n",
       "  [291, 292],\n",
       "  [292, 293],\n",
       "  [294, 295],\n",
       "  [295, 296],\n",
       "  [296, 297],\n",
       "  [298, 299],\n",
       "  [299, 302],\n",
       "  [302, 305],\n",
       "  [305, 309],\n",
       "  [310, 312],\n",
       "  [312, 316],\n",
       "  [316, 317],\n",
       "  [318, 319],\n",
       "  [319, 322],\n",
       "  [322, 324],\n",
       "  [324, 325],\n",
       "  [326, 327],\n",
       "  [327, 328],\n",
       "  [328, 329],\n",
       "  [329, 332],\n",
       "  [333, 334],\n",
       "  [335, 341],\n",
       "  [342, 348],\n",
       "  [349, 353],\n",
       "  [354, 362],\n",
       "  [362, 363],\n",
       "  [364, 365],\n",
       "  [366, 368],\n",
       "  [368, 371],\n",
       "  [371, 375],\n",
       "  [376, 381],\n",
       "  [382, 383],\n",
       "  [383, 384],\n",
       "  [384, 387],\n",
       "  [387, 389],\n",
       "  [390, 391],\n",
       "  [392, 393],\n",
       "  [393, 398],\n",
       "  [399, 404],\n",
       "  [405, 406],\n",
       "  [407, 409],\n",
       "  [409, 410],\n",
       "  [410, 413],\n",
       "  [414, 416],\n",
       "  [416, 417],\n",
       "  [418, 420],\n",
       "  [421, 422],\n",
       "  [422, 423],\n",
       "  [424, 425],\n",
       "  [425, 426],\n",
       "  [426, 427],\n",
       "  [428, 436],\n",
       "  [436, 438],\n",
       "  [439, 446],\n",
       "  [447, 449],\n",
       "  [449, 452],\n",
       "  [453, 455],\n",
       "  [455, 457],\n",
       "  [457, 460],\n",
       "  [460, 466],\n",
       "  [467, 473],\n",
       "  [474, 475],\n",
       "  [476, 478],\n",
       "  [478, 479],\n",
       "  [479, 482],\n",
       "  [482, 483],\n",
       "  [484, 486],\n",
       "  [486, 489],\n",
       "  [489, 490],\n",
       "  [490, 491],\n",
       "  [492, 496],\n",
       "  [496, 497],\n",
       "  [498, 502],\n",
       "  [502, 505],\n",
       "  [505, 510],\n",
       "  [511, 512],\n",
       "  [513, 516],\n",
       "  [516, 520],\n",
       "  [520, 525],\n",
       "  [525, 526],\n",
       "  [527, 532],\n",
       "  [532, 535],\n",
       "  [536, 537],\n",
       "  [538, 539],\n",
       "  [539, 541],\n",
       "  [541, 544],\n",
       "  [544, 545],\n",
       "  [546, 547],\n",
       "  [547, 549],\n",
       "  [549, 551],\n",
       "  [551, 552],\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None],\n",
       " 'start_positions': 33,\n",
       " 'end_positions': 40,\n",
       " 'example_id': 60544}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.set_format(\"torch\")\n",
    "\n",
    "dataset_val_formatted = dataset_val.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "dataset_val_formatted.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val_formatted,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функций для оценки тренировки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(start_logits, end_logits, inputs, examples, n_best=20, max_answer_length=30):\n",
    "    assert n_best <= len(inputs['offset_mapping'][0]), 'n_best cannot be larger than max_length'\n",
    "    \n",
    "    example_to_inputs = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(inputs):\n",
    "        example_to_inputs[str(feature[\"example_id\"])].append(idx)\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = str(example[\"id\"])\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        for feature_index in example_to_inputs[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "\n",
    "            offsets = inputs[feature_index]['offset_mapping']\n",
    "            start_indices = np.argsort(start_logit)[-1:-n_best-1:-1].tolist()\n",
    "            end_indices = np.argsort(end_logit)[-1 :-n_best-1: -1].tolist()\n",
    "\n",
    "            for start_index in start_indices:\n",
    "                for end_index in end_indices:\n",
    "                    if (end_index < start_index or end_index - start_index + 1 > max_answer_length):\n",
    "                        continue\n",
    "                    if (offsets[start_index] is None)^(offsets[end_index] is None):\n",
    "                        continue\n",
    "                    \n",
    "                    if (offsets[start_index] is None)&(offsets[end_index] is None):\n",
    "                        answers.append(\n",
    "                            {\n",
    "                                \"text\": '',\n",
    "                                \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "                        answers.append(\n",
    "                            {\n",
    "                                \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                                \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x:x['logit_score'])\n",
    "            predicted_answers.append({'id':example_id, 'prediction_text':best_answer['text']})\n",
    "        else:\n",
    "            predicted_answers.append({'id':example_id, 'prediction_text':''})\n",
    "\n",
    "    return predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, inputs, examples, n_best = 20, max_answer_length=30):\n",
    "\n",
    "    metric = evaluate.load('squad_v2')\n",
    "    predicted_answers = format_predictions(start_logits, end_logits, inputs, examples,\n",
    "                                           n_best=n_best, max_answer_length=max_answer_length)\n",
    "    for pred in predicted_answers:\n",
    "        pred['no_answer_probability'] = 1.0 if pred['prediction_text'] == '' else 0.0\n",
    "\n",
    "    correct_answers = []\n",
    "    for example in examples:\n",
    "        input_id = str(example[\"id\"])\n",
    "        answers = example[\"answers\"]\n",
    "        correct_answers.append({\n",
    "            \"id\": input_id,\n",
    "            \"answers\": {\n",
    "                \"text\": answers[\"text\"],\n",
    "                \"answer_start\": answers[\"answer_start\"]\n",
    "            }\n",
    "        })\n",
    "    return metric.compute(predictions=predicted_answers, references=correct_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "num_epochs = 2\n",
    "num_training_steps = len(dataloader_train)*num_epochs\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\python_projects\\ru_qa_system\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, dataloader_train, dataloader_val = accelerator.prepare(\n",
    "    model, optimizer, dataloader_train, dataloader_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1143c89d144b4acf8ed773d8987d4ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5974 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47213e78b0d94355bd6f255a83fe48f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Оценка модели:   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af54e89e4a724372b2e28a8c13387872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: {'exact': 53.05798252581414, 'f1': 73.12466310014788, 'total': 5036, 'HasAns_exact': 53.05798252581414, 'HasAns_f1': 73.12466310014788, 'HasAns_total': 5036, 'best_exact': 53.05798252581414, 'best_exact_thresh': 0.0, 'best_f1': 73.12466310014788, 'best_f1_thresh': 0.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0adfa0525cef49668f060139489d15c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Оценка модели:   0%|          | 0/333 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c463c168ab7242f3a6f78bed39fbaf6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 2: {'exact': 54.76568705321684, 'f1': 74.41162288203093, 'total': 5036, 'HasAns_exact': 54.76568705321684, 'HasAns_f1': 74.41162288203093, 'HasAns_total': 5036, 'best_exact': 54.76568705321684, 'best_exact_thresh': 0.0, 'best_f1': 74.41162288203093, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Тренировка модели\n",
    "    model.train()\n",
    "    for step, batch in enumerate(dataloader_train):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Оценка на валидационных данных\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for batch in tqdm(dataloader_val, desc='Оценка модели'):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "    \n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(dataset_val)]\n",
    "    end_logits = end_logits[: len(dataset_val)]\n",
    "    \n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, dataset_val, dataset['validation']\n",
    "    )\n",
    "    print(f\"Эпоха {epoch+1}:\", metrics)\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(MODEL_SAVE_DIR,save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие метрики были получены на 2ой эпохе:\n",
    "\n",
    "> Exact match = 54.77 \\\n",
    "> F1-score = 74.41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(MODEL_SAVE_DIR)\n",
    "model.save_pretrained(MODEL_SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
