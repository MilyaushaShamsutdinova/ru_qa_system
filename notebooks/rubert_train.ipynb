{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сетап"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, get_scheduler, default_data_collator\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import collections\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"kuznetsoffandrey/sberquad\"\n",
    "MODEL_NAME = \"DeepPavlov/rubert-base-cased\"\n",
    "MODEL_SAVE_DIR ='rubert-v3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка датасета и его предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 45328\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 5036\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 23936\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(examples, tokenizer, is_test=False, max_length=384, stride=128):\n",
    "    questions = [q.strip() for q in examples['question']]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    \n",
    "    answers = examples[\"answers\"]\n",
    "    if is_test:\n",
    "        offset_mapping = inputs[\"offset_mapping\"]\n",
    "        example_ids = []\n",
    "    else:\n",
    "        offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        offset = offset_mapping[i]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        if is_test:\n",
    "            example_ids.append(examples[\"id\"][sample_idx])\n",
    "            inputs[\"offset_mapping\"][i] = [\n",
    "                o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "            ]\n",
    "\n",
    "        if len(answer['answer_start'])==0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "        \n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        \n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "            \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    if is_test:\n",
    "        inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset['train'].map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['train'].column_names,\n",
    "    fn_kwargs = {\n",
    "        'tokenizer': tokenizer,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'start_positions', 'end_positions'],\n",
       "    num_rows: 45544\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d61093263ffd44ca81dc86514c2e6b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5036 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_val = dataset['validation'].map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    remove_columns=dataset['validation'].column_names,\n",
    "    fn_kwargs = {\n",
    "        'tokenizer': tokenizer,\n",
    "        'is_test': True,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.set_format(\"torch\")\n",
    "\n",
    "dataset_val_formatted = dataset_val.remove_columns([\"example_id\", \"offset_mapping\"])\n",
    "dataset_val_formatted.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "dataloader_val = DataLoader(\n",
    "    dataset_val_formatted,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Определение функций для оценки тренировки модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_predictions(start_logits, end_logits, inputs, examples, n_best=20, max_answer_length=30):\n",
    "    assert n_best <= len(inputs['offset_mapping'][0]), 'n_best cannot be larger than max_length'\n",
    "    \n",
    "    example_to_inputs = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(inputs):\n",
    "        example_to_inputs[str(feature[\"example_id\"])].append(idx)\n",
    "    \n",
    "    predicted_answers = []\n",
    "    for example in tqdm(examples):\n",
    "        example_id = str(example[\"id\"])\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "        \n",
    "        for feature_index in example_to_inputs[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "\n",
    "            offsets = inputs[feature_index]['offset_mapping']\n",
    "            start_indices = np.argsort(start_logit)[-1:-n_best-1:-1].tolist()\n",
    "            end_indices = np.argsort(end_logit)[-1 :-n_best-1: -1].tolist()\n",
    "\n",
    "            for start_index in start_indices:\n",
    "                for end_index in end_indices:\n",
    "                    if (end_index < start_index or end_index - start_index + 1 > max_answer_length):\n",
    "                        continue\n",
    "                    if (offsets[start_index] is None)^(offsets[end_index] is None):\n",
    "                        continue\n",
    "                    \n",
    "                    if (offsets[start_index] is None)&(offsets[end_index] is None):\n",
    "                        answers.append(\n",
    "                            {\n",
    "                                \"text\": '',\n",
    "                                \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "                        answers.append(\n",
    "                            {\n",
    "                                \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                                \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x:x['logit_score'])\n",
    "            predicted_answers.append({'id':example_id, 'prediction_text':best_answer['text']})\n",
    "        else:\n",
    "            predicted_answers.append({'id':example_id, 'prediction_text':''})\n",
    "\n",
    "    return predicted_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(start_logits, end_logits, inputs, examples, n_best = 20, max_answer_length=30):\n",
    "\n",
    "    metric = evaluate.load('squad_v2')\n",
    "    predicted_answers = format_predictions(start_logits, end_logits, inputs, examples,\n",
    "                                           n_best=n_best, max_answer_length=max_answer_length)\n",
    "    for pred in predicted_answers:\n",
    "        pred['no_answer_probability'] = 1.0 if pred['prediction_text'] == '' else 0.0\n",
    "\n",
    "    correct_answers = []\n",
    "    for example in examples:\n",
    "        input_id = str(example[\"id\"])\n",
    "        answers = example[\"answers\"]\n",
    "        correct_answers.append({\n",
    "            \"id\": input_id,\n",
    "            \"answers\": {\n",
    "                \"text\": answers[\"text\"],\n",
    "                \"answer_start\": answers[\"answer_start\"]\n",
    "            }\n",
    "        })\n",
    "    return metric.compute(predictions=predicted_answers, references=correct_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "num_epochs = 1\n",
    "num_training_steps = len(dataloader_train)*num_epochs\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    'linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps = 0,\n",
    "    num_training_steps = num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\python_projects\\ru_qa_system\\venv\\Lib\\site-packages\\accelerate\\accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "\n",
    "model, optimizer, dataloader_train, dataloader_val = accelerator.prepare(\n",
    "    model, optimizer, dataloader_train, dataloader_val\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6073416609a342debfc1d105cf97213a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2847 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\python_projects\\ru_qa_system\\venv\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eca4bb75387405bb5fa9a13fd93b19f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Оценка модели:   0%|          | 0/317 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6699a89930bb4c6db9a5be7b9504f242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1: {'exact': 62.4900714853058, 'f1': 82.10095375768547, 'total': 5036, 'HasAns_exact': 62.4900714853058, 'HasAns_f1': 82.10095375768547, 'HasAns_total': 5036, 'best_exact': 62.4900714853058, 'best_exact_thresh': 0.0, 'best_f1': 82.10095375768547, 'best_f1_thresh': 0.0}\n"
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Тренировка модели\n",
    "    model.train()\n",
    "    for step, batch in enumerate(dataloader_train):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "        \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    # Оценка на валидационных данных\n",
    "    model.eval()\n",
    "    start_logits = []\n",
    "    end_logits = []\n",
    "    for batch in tqdm(dataloader_val, desc='Оценка модели'):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        start_logits.append(accelerator.gather(outputs.start_logits).cpu().numpy())\n",
    "        end_logits.append(accelerator.gather(outputs.end_logits).cpu().numpy())\n",
    "    \n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    start_logits = start_logits[: len(dataset_val)]\n",
    "    end_logits = end_logits[: len(dataset_val)]\n",
    "    \n",
    "    metrics = compute_metrics(\n",
    "        start_logits, end_logits, dataset_val, dataset['validation']\n",
    "    )\n",
    "    print(f\"Эпоха {epoch+1}:\", metrics)\n",
    "    \n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(MODEL_SAVE_DIR,save_function=accelerator.save)\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(MODEL_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие метрики были получены на 1ой эпохе:\n",
    "\n",
    "> Exact match = 62.49 \\\n",
    "> F1-score = 82.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(MODEL_SAVE_DIR)\n",
    "model.save_pretrained(MODEL_SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
